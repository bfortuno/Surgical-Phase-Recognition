{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bfortuno/Surgical-Phase-Recognition/blob/main/docs/tutorial_notebooks/tutorial6/misaw_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. üì¶ MISAW Dataset Preprocessing\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View filled on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/bfortuno/Surgical-Phase-Recognition/blob/main/docs/tutorial_notebooks/tutorial3/pytorch_intro.ipynb)\n",
        "[![Open filled In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bfortuno/Surgical-Phase-Recognition/blob/main/docs/tutorial_notebooks/tutorial4/misaw_data_processing.ipynb)    \n",
        "**Author:** Benjamin I. Fortuno"
      ],
      "metadata": {
        "id": "0VDaCxqgOJ5r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1807621"
      },
      "source": [
        "This notebook is intended to preprocess data from the MISAW dataset, commonly used for computer vision and machine learning tasks. It covers:\n",
        "\n",
        "- Downloading and extracting the dataset\n",
        "- Exploring the data structure\n",
        "- Parsing image and label files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcdce6df"
      },
      "source": [
        "### üß∞ Importing Required Libraries\n",
        "This section loads essential Python libraries like `os`, `cv2`, `glob`, and `pandas` which are needed for handling files, images, and data manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mxn02EuSrmdh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "840ba669"
      },
      "source": [
        "### üì• Downloading the Dataset\n",
        "Here we download the MISAW dataset in `.zip` format from a Dropbox link. This dataset contains videos and annotations for surgical workflow analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pihxeq42rrJ4",
        "outputId": "80f240a6-2479-4e72-bbfb-5af5ae2ec677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-10 08:48:47--  https://www.dropbox.com/scl/fi/psmlokrc5ms958ggqyv3u/MISAW.zip?rlkey=v91dz437npon5zz10olrbcqcd\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com/cd/0/inline/CniaYHfd0OgnFwNgdXAsBPj6QKSQdhKx2sjKIOe7Z_JaP8arI3YdFlKq4sT3HkKA-QhunTpFBzsMExf_2iEd1veiUD0o_2HGpHUhQa2UJg-HIvSbtmbGt72SHQJvfcOUqbvfbTLdEd_VXdgFY9Tk6Mg6/file# [following]\n",
            "--2025-04-10 08:48:48--  https://uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com/cd/0/inline/CniaYHfd0OgnFwNgdXAsBPj6QKSQdhKx2sjKIOe7Z_JaP8arI3YdFlKq4sT3HkKA-QhunTpFBzsMExf_2iEd1veiUD0o_2HGpHUhQa2UJg-HIvSbtmbGt72SHQJvfcOUqbvfbTLdEd_VXdgFY9Tk6Mg6/file\n",
            "Resolving uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com (uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com (uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cni0rpMfinkgkZULH4Z73ji1P9UhYsQ-sBBbUIuJrt-tD87HQtOJk7xE2t041q3XbwW9P7RI922lm-xJ-x5-77DIKaMs-xeEUW5N3s0flYVbi3SHifPknIYLLJ8L1ZfaY5y5w5ljjF3TWClrLUKIJXVmrJHppYw7OWcroq-M7WFabYK7vhHss_TtlmA3VnLLMAe8AJkuQgyrX8959a-LjB9nUm3liUvuquX9kS9UF-sg6YjpHIOzwYuzcBb2WDhbE6_nB1hfS_WCEP_hKhQQ6m7LtYbKwF9RbQBIbIkniJPYpUWgSAxSkwJ6di9ABsgRx8MajEvEAEGQhc3FyHqCJRQncQbIDDRJvL3xxOULOFWOvm4O5U3EAaenfvV0Lo79bzg/file [following]\n",
            "--2025-04-10 08:48:48--  https://uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com/cd/0/inline2/Cni0rpMfinkgkZULH4Z73ji1P9UhYsQ-sBBbUIuJrt-tD87HQtOJk7xE2t041q3XbwW9P7RI922lm-xJ-x5-77DIKaMs-xeEUW5N3s0flYVbi3SHifPknIYLLJ8L1ZfaY5y5w5ljjF3TWClrLUKIJXVmrJHppYw7OWcroq-M7WFabYK7vhHss_TtlmA3VnLLMAe8AJkuQgyrX8959a-LjB9nUm3liUvuquX9kS9UF-sg6YjpHIOzwYuzcBb2WDhbE6_nB1hfS_WCEP_hKhQQ6m7LtYbKwF9RbQBIbIkniJPYpUWgSAxSkwJ6di9ABsgRx8MajEvEAEGQhc3FyHqCJRQncQbIDDRJvL3xxOULOFWOvm4O5U3EAaenfvV0Lo79bzg/file\n",
            "Reusing existing connection to uc9b6ff395a43e6f38bcc2d565c0.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 965450623 (921M) [application/zip]\n",
            "Saving to: ‚ÄòMISAW.zip‚Äô\n",
            "\n",
            "MISAW.zip           100%[===================>] 920.72M  56.7MB/s    in 15s     \n",
            "\n",
            "2025-04-10 08:49:04 (59.6 MB/s) - ‚ÄòMISAW.zip‚Äô saved [965450623/965450623]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O MISAW.zip https://www.dropbox.com/scl/fi/psmlokrc5ms958ggqyv3u/MISAW.zip?rlkey=v91dz437npon5zz10olrbcqcd&st=54qvf31m&dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79da29e9"
      },
      "source": [
        "### üì¶ Extracting the Dataset\n",
        "This cell unzips the downloaded file to access the raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tfaLOhLOxqri"
      },
      "outputs": [],
      "source": [
        "!unzip -qq MISAW.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b809ff4"
      },
      "source": [
        "## üéûÔ∏è Frame Extraction from Videos\n",
        "This section reads the videos and extracts every N-th frame (controlled by `resample_rate`).\n",
        "Each video gets its own subdirectory of frames, organized as:\n",
        "```\n",
        "MISAW/\n",
        "  ‚îî‚îÄ‚îÄ train/\n",
        "        ‚îî‚îÄ‚îÄ Frames/\n",
        "              ‚îî‚îÄ‚îÄ <video_id>/frame_0000.jpg\n",
        "```\n",
        "These images will be later paired with annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ4dwnDizRPS",
        "outputId": "005d2f3f-0743-4050-96dc-51f46bed52c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 36 frames from 6_4.mp4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2  # OpenCV is used for handling video files and frame extraction\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1. Configuration\n",
        "# ----------------------------------------\n",
        "\n",
        "# Choose how often to save a frame (e.g., every 120th frame)\n",
        "# A higher number results in fewer frames being saved. Start high for fast testing.\n",
        "resample_rate = 120\n",
        "\n",
        "# Path to the folder containing input videos\n",
        "video_folder = 'MISAW/train/Video'\n",
        "\n",
        "# Path where extracted frames will be saved\n",
        "frames_folder = 'MISAW/train/Frames'\n",
        "\n",
        "# Create the frames folder if it doesn't exist\n",
        "os.makedirs(frames_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Process each video file in the folder\n",
        "# ----------------------------------------\n",
        "\n",
        "for video_file in os.listdir(video_folder):\n",
        "    # Check for supported video formats\n",
        "    if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "        video_path = os.path.join(video_folder, video_file)  # Full path to video file\n",
        "        video_name = os.path.splitext(video_file)[0]         # Extract video name without extension\n",
        "\n",
        "        # Create a subfolder for the frames of this video\n",
        "        video_frames_path = os.path.join(frames_folder, video_name)\n",
        "        os.makedirs(video_frames_path, exist_ok=True)\n",
        "\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Counters to keep track of frames read and saved\n",
        "        frame_count = 0      # Total number of frames read\n",
        "        saved_count = 0      # Number of frames saved\n",
        "\n",
        "        # ----------------------------------------\n",
        "        # 3. Read frames one-by-one\n",
        "        # ----------------------------------------\n",
        "\n",
        "        while True:\n",
        "            success, frame = cap.read()  # Read one frame\n",
        "            if not success:\n",
        "                break  # End of video\n",
        "\n",
        "            # Save every N-th frame (based on resample_rate)\n",
        "            if frame_count % resample_rate == 0:\n",
        "                # Save frame with a 4-digit padded name (e.g., frame_0001.jpg)\n",
        "                # `:04d` formats the integer with 4 digits, padded with zeros\n",
        "                frame_name = f\"frame_{saved_count:04d}.jpg\"\n",
        "\n",
        "                # Full path to save the frame\n",
        "                frame_path = os.path.join(video_frames_path, frame_name)\n",
        "\n",
        "                # Save the frame as an image file\n",
        "                cv2.imwrite(frame_path, frame)\n",
        "\n",
        "                # Increment saved frame counter\n",
        "                saved_count += 1\n",
        "\n",
        "            # Increment total frame counter\n",
        "            frame_count += 1\n",
        "\n",
        "        # Release the video capture object\n",
        "        cap.release()\n",
        "\n",
        "        # Print a summary for this video\n",
        "        print(f\"Saved {saved_count} frames from {video_file}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. Final message\n",
        "# ----------------------------------------\n",
        "\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WTaT0NgE1cF"
      },
      "source": [
        "### üí° Explanation: f-strings and formatting\n",
        "\n",
        "#### **What is an f-string?**\n",
        "An **f-string** allows inline expressions in strings:\n",
        "```python\n",
        "name = \"Benjamin\"\n",
        "print(f\"Hello, {name}!\")  # Outputs: Hello, Benjamin!\n",
        "```\n",
        "\n",
        "#### **Formatting numbers: `:04d`**\n",
        "When saving frame names, we want them **zero-padded to 4 digits** so they're sorted correctly:\n",
        "```python\n",
        "frame_num = 7\n",
        "print(f\"frame_{frame_num:04d}.jpg\")  # Outputs: frame_0007.jpg\n",
        "```\n",
        "\n",
        "- `0` ‚Üí pad with zeros  \n",
        "- `4` ‚Üí total length of 4 digits  \n",
        "- `d` ‚Üí it's a decimal (integer)  \n",
        "\n",
        "This ensures your filenames look like: `frame_0000.jpg`, `frame_0001.jpg`, ..., `frame_0123.jpg`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17133f20"
      },
      "source": [
        "## üßæ Parsing Annotations and Building Frame-Label Pairs\n",
        "Here we parse the `.txt` annotation files (one per video) to extract surgical phases, then pair each extracted frame with a corresponding label.\n",
        "We also map textual phase labels to numeric IDs, which is important for training machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jMF8NXY0Z8F"
      },
      "outputs": [],
      "source": [
        "# Folder where the annotation .txt files are stored (one per video)\n",
        "annotation_folder = 'MISAW/train/Procedural decription'\n",
        "\n",
        "# Folder where extracted frames from videos are stored (in subfolders per video)\n",
        "frames_folder = 'MISAW/train/Frames'\n",
        "\n",
        "# This is the same rate you used to extract frames from videos (e.g., every 120th frame)\n",
        "# It must match or your labels won't align with the frames!\n",
        "resample_rate = 120\n",
        "\n",
        "\n",
        "# --- Collect all unique phases first ---\n",
        "\n",
        "# We'll store all unique phase labels (like \"Idle\", \"Suturing\", etc.) in this set\n",
        "all_phases = set()\n",
        "\n",
        "# Find all annotation files in the folder (e.g., 1_1_annotation.txt, 1_2_annotation.txt, etc.)\n",
        "annotation_files = sorted(glob.glob(os.path.join(annotation_folder, '*_annotation.txt')))\n",
        "\n",
        "# Loop through each annotation file and collect unique phase names\n",
        "for anno_file in annotation_files:\n",
        "    df = pd.read_csv(anno_file, sep='\\t')  # Load the .txt file into a DataFrame\n",
        "    all_phases.update(df['Phase'].unique())  # Add unique phases to the set\n",
        "\n",
        "# Create a dictionary to map each phase string to a unique integer ID\n",
        "# Useful for training machine learning models\n",
        "phase_to_id = {name: i for i, name in enumerate(sorted(all_phases))}\n",
        "\n",
        "# Create the inverse mapping (ID to phase name) for visualization later\n",
        "id_to_phase = {i: name for name, i in phase_to_id.items()}\n",
        "\n",
        "\n",
        "# --- Build (frame_path, label) pairs ---\n",
        "\n",
        "# This list will hold tuples like: (\"path/to/frame.jpg\", 2) ‚Üí (frame, label_id)\n",
        "all_data = []\n",
        "\n",
        "# Go through each annotation file (one per video)\n",
        "for anno_file in annotation_files:\n",
        "    # Extract the video ID from the filename (e.g., \"1_1_annotation.txt\" ‚Üí \"1_1\")\n",
        "    video_id = os.path.basename(anno_file).replace('_annotation.txt', '')\n",
        "\n",
        "    # Construct the path to the corresponding frame folder\n",
        "    frame_dir = os.path.join(frames_folder, video_id)\n",
        "\n",
        "    # Read the annotation file into a DataFrame\n",
        "    df = pd.read_csv(anno_file, sep='\\t')\n",
        "\n",
        "    # Get the full list of phases (one per original video frame)\n",
        "    phases = df['Phase'].tolist()\n",
        "\n",
        "    # Resample: only keep every N-th label (e.g., every 120th label)\n",
        "    sampled_phases = phases[::resample_rate]\n",
        "\n",
        "    # Convert phase strings to numeric labels using our earlier mapping\n",
        "    sampled_ids = [phase_to_id[p] for p in sampled_phases]\n",
        "\n",
        "    # Get the list of frame image paths, sorted so they match the order of labels\n",
        "    frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "\n",
        "    # Sanity check: if the number of frames doesn‚Äôt match the number of labels, skip this video\n",
        "    if len(frame_paths) != len(sampled_ids):\n",
        "        print(f\"‚ö†Ô∏è Mismatch for {video_id}: {len(frame_paths)} frames vs {len(sampled_ids)} labels\")\n",
        "        continue\n",
        "\n",
        "    # Add all (frame_path, label_id) pairs to our global list\n",
        "    all_data.extend(zip(frame_paths, sampled_ids))\n",
        "\n",
        "\n",
        "# Final print to confirm total number of samples loaded\n",
        "print(f\"‚úÖ Total samples: {len(all_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize images and phases"
      ],
      "metadata": {
        "id": "pexU3fsGGdeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "# Number of sample images to display per phase (i.e., class/label)\n",
        "samples_per_phase = 3\n",
        "\n",
        "# Create a dictionary to group image paths by their label (phase ID)\n",
        "# defaultdict(list) automatically creates an empty list for new keys\n",
        "label_to_paths = defaultdict(list)\n",
        "\n",
        "# all_data is assumed to be a list of (frame_path, label_id) pairs\n",
        "# Here we organize all frame paths under their respective label IDs\n",
        "for path, label_id in all_data:\n",
        "    label_to_paths[label_id].append(path)\n",
        "\n",
        "# Determine the number of unique phases (rows in the final plot)\n",
        "n_rows = len(label_to_paths)\n",
        "\n",
        "# Number of columns equals the number of samples we want to show per phase\n",
        "n_cols = samples_per_phase\n",
        "\n",
        "# Create a grid of subplots (n_rows x n_cols)\n",
        "# figsize sets the overall size of the figure (in inches)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "# Add a main title above all the subplots\n",
        "fig.suptitle(\"Random Samples per Phase\", fontsize=18)\n",
        "\n",
        "# Get all label IDs in sorted order (for consistent row display)\n",
        "sorted_labels = sorted(label_to_paths.keys())\n",
        "\n",
        "# Loop over each row (i.e., each unique label/phase)\n",
        "for row, label_id in enumerate(sorted_labels):\n",
        "    # Convert label ID back to its readable name using id_to_phase mapping\n",
        "    label_name = id_to_phase[label_id]\n",
        "\n",
        "    # Randomly sample a few images from this label\n",
        "    # Ensure we don't try to sample more images than we actually have\n",
        "    samples = random.sample(label_to_paths[label_id], min(samples_per_phase, len(label_to_paths[label_id])))\n",
        "\n",
        "    # For each column (i.e., each sampled image for this label)\n",
        "    for col in range(samples_per_phase):\n",
        "        # Access the correct subplot (row x col)\n",
        "        # If there's only one row, axes may be 1D\n",
        "        ax = axes[row, col] if n_rows > 1 else axes[col]\n",
        "\n",
        "        # Only plot if we have enough samples for this column\n",
        "        if col < len(samples):\n",
        "            # Open the image file using PIL\n",
        "            img = Image.open(samples[col])\n",
        "\n",
        "            # Display the image in the subplot\n",
        "            ax.imshow(img)\n",
        "\n",
        "            # Set the subplot title to the phase name\n",
        "            ax.set_title(f\"{label_name}\", fontsize=10)\n",
        "\n",
        "        # Remove axis ticks and labels for a cleaner look\n",
        "        ax.axis('off')\n",
        "\n",
        "# Adjust layout to prevent overlaps\n",
        "plt.tight_layout()\n",
        "\n",
        "# Adjust top spacing to make room for the main title\n",
        "plt.subplots_adjust(top=0.95)\n",
        "\n",
        "# Display the final grid of images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ijabIfAGcdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù TODO: Customize the Annotation Granularity\n",
        "\n",
        "By default, the code extracts labels from the `\"Phase\"` column in the annotation files.  \n",
        "However, you can modify this to use a different level of granularity depending on your task (e.g. `\"Verb_Left\"`, `\"Tool_Right\"`, etc.).\n",
        "\n",
        "**To do:**  \n",
        "Replace `\"Phase\"` with the desired column name when reading annotations.\n",
        "\n",
        "**Where to look:**  \n",
        "Open any `*_annotation.txt` file inside the folder:  \n",
        "`MISAW/train/Procedural description/`  \n",
        "This will show you the available columns you can use for training with different labels."
      ],
      "metadata": {
        "id": "yIxmpLaSIob5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÅ Dataset Formats for Surgical Phase Recognition\n",
        "\n",
        "In this notebook, we explore **three common dataset formats** used in computer vision and deep learning. All formats are based on the original `MISAW/train/Frames` and `MISAW/train/Procedural description` data, and are designed to support different model architectures and training needs:\n",
        "\n",
        "### 1. **COCO-style (JSON-based)**  \n",
        "Stores all annotations in a single `annotations.json` file, where each entry includes the image path, video ID, frame name, and label.  \n",
        "‚úÖ Great for flexible pipelines, multi-task learning, and custom datasets.\n",
        "\n",
        "### 2. **ImageNet-style (Folder-based)**  \n",
        "Organizes frames into folders named after their label (e.g., `Suturing/`, `Idle/`, etc.), as expected by libraries like `torchvision.datasets.ImageFolder`.  \n",
        "‚úÖ Ideal for standard CNN classification pipelines.\n",
        "\n",
        "### 3. **CSV-based (Flat Index)**  \n",
        "Stores all frame paths and labels in a single `annotations.csv` file, which can be loaded with `pandas` for quick filtering, sampling, and data manipulation.  \n",
        "‚úÖ Best for prototyping and Pandas-based pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "Each format comes with TODO sections in the scripts ‚Äî the idea is for you to:\n",
        "\n",
        "- üîß **Fill in the missing parts** (e.g., replace `\"Phase\"` with another label like `\"Step\"`)\n",
        "- ‚úÖ **Use the validation cells** provided to check if the format is correctly built\n",
        "\n",
        "Once completed, these formats will allow you to train and evaluate models easily using PyTorch or other frameworks."
      ],
      "metadata": {
        "id": "gNZGAMbMLkAH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997a927a"
      },
      "source": [
        "### üóÇÔ∏è Converting to ImageNet-style Format\n",
        "Organizes the dataset in a class-wise folder structure suitable for image classification tasks, like this:\n",
        "```\n",
        "MISAW_imnet/\n",
        "  ‚îú‚îÄ‚îÄ Phase1/\n",
        "  ‚îÇ     ‚îî‚îÄ‚îÄ video1_frame_0000.jpg\n",
        "  ‚îî‚îÄ‚îÄ Phase2/\n",
        "        ‚îî‚îÄ‚îÄ video2_frame_0000.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxOrMhZc_Lpl"
      },
      "outputs": [],
      "source": [
        "import os           # For directory operations\n",
        "import glob         # For listing files using patterns\n",
        "import pandas as pd # For reading annotation files\n",
        "import shutil       # For copying images\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "resample_rate = 120  # Use the same value you used when extracting frames\n",
        "\n",
        "# Path to folder containing annotation files (one .txt file per video)\n",
        "annotation_folder = 'MISAW/train/Procedural decription'\n",
        "\n",
        "# Path to the folder where extracted frames are stored\n",
        "frames_folder = 'MISAW/train/Frames'\n",
        "\n",
        "# Path to the output ImageNet-style dataset\n",
        "output_root = 'MISAW_imnet'\n",
        "os.makedirs(output_root, exist_ok=True)  # Create output folder if it doesn't exist\n",
        "\n",
        "# --- BUILD IMAGE FOLDER STRUCTURE BY PHASE (LABEL) ---\n",
        "\n",
        "# Loop through all annotation files (e.g., 1_1_annotation.txt, 1_2_annotation.txt, ...)\n",
        "for anno_file in glob.glob(f'{annotation_folder}/*_annotation.txt'):\n",
        "\n",
        "    # Extract the video ID (e.g., \"1_1\" from \"1_1_annotation.txt\")\n",
        "    video_id = os.path.basename(anno_file).replace('_annotation.txt', '')\n",
        "\n",
        "    # Read the annotation file as a DataFrame\n",
        "    df = pd.read_csv(anno_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Phase' column and resample it (e.g., every 120th label)\n",
        "    phases = df['Phase'].tolist()[::resample_rate]\n",
        "\n",
        "    # Get the corresponding frame image paths (already extracted frames)\n",
        "    frame_paths = sorted(glob.glob(f\"{frames_folder}/{video_id}/*.jpg\"))\n",
        "\n",
        "    # Check that the number of labels matches the number of frames\n",
        "    if len(phases) != len(frame_paths):\n",
        "        print(f\"‚ö†Ô∏è Skipping {video_id}: mismatched {len(phases)} labels vs {len(frame_paths)} frames\")\n",
        "        continue  # Skip this video if the lengths don't match\n",
        "\n",
        "    # For each (frame, phase) pair\n",
        "    for i, (frame_path, phase) in enumerate(zip(frame_paths, phases)):\n",
        "\n",
        "        # Create the folder for this phase if it doesn't exist\n",
        "        phase_folder = os.path.join(output_root, phase)\n",
        "        os.makedirs(phase_folder, exist_ok=True)\n",
        "\n",
        "        # Create a consistent new filename (e.g., \"1_1_frame_0004.jpg\")\n",
        "        new_name = f\"{video_id}_frame_{i:04d}.jpg\"\n",
        "\n",
        "        # Copy the frame to the corresponding phase folder\n",
        "        shutil.copy(frame_path, os.path.join(phase_folder, new_name))\n",
        "\n",
        "# Final message\n",
        "print(f\"‚úÖ ImageNet-style folders created in {output_root}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation"
      ],
      "metadata": {
        "id": "5ILM8l_lT2m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Path to ImageNet root\n",
        "imnet_root = 'MISAW_imnet'\n",
        "phases = [d for d in os.listdir(imnet_root) if os.path.isdir(os.path.join(imnet_root, d))]\n",
        "assert phases, \"‚ùå No class folders found in ImageNet-style structure\"\n",
        "\n",
        "# Collect file counts per phase\n",
        "phase_counts = defaultdict(int)\n",
        "total = 0\n",
        "\n",
        "for phase in phases:\n",
        "    path = os.path.join(imnet_root, phase)\n",
        "    files = [f for f in os.listdir(path) if f.endswith(('.jpg', '.png'))]\n",
        "    phase_counts[phase] = len(files)\n",
        "    total += len(files)\n",
        "\n",
        "print(f\"‚úÖ Found {len(phases)} phases with a total of {total} images\")\n",
        "\n",
        "# Show sample per phase\n",
        "for phase in sorted(phase_counts.keys())[:3]:\n",
        "    print(f\"üîç {phase}: {phase_counts[phase]} images\")\n",
        "    sample = os.listdir(os.path.join(imnet_root, phase))[0]\n",
        "    print(f\"   e.g., {sample}\")\n"
      ],
      "metadata": {
        "id": "OCwqdgeMK5Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caf59dce"
      },
      "source": [
        "### üìÑ Converting to CSV-style Format\n",
        "Stores the dataset in a flat folder of frames and uses a CSV file to keep track of the mapping between frame paths and their class labels:\n",
        "```\n",
        "MISAW_csv/\n",
        "  ‚îú‚îÄ‚îÄ Frames/\n",
        "  ‚îÇ     ‚îî‚îÄ‚îÄ video1_frame_0000.jpg\n",
        "  ‚îî‚îÄ‚îÄ annotations.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLrNC5qQ_zNx"
      },
      "outputs": [],
      "source": [
        "import os           # For working with directories\n",
        "import glob         # For listing files with wildcard patterns\n",
        "import pandas as pd # For reading/writing CSV and annotation files\n",
        "import shutil       # For copying image files\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "resample_rate = 120  # Must match the rate used when extracting frames\n",
        "\n",
        "# Folder where annotation .txt files are stored (e.g., \"1_1_annotation.txt\")\n",
        "annotation_folder = 'MISAW/train/Procedural decription'\n",
        "\n",
        "# Folder where extracted frames are stored (per video folder)\n",
        "frames_folder = 'MISAW/train/Frames'\n",
        "\n",
        "# Folder where the final CSV-based dataset will be saved\n",
        "output_root = 'MISAW_csv'\n",
        "\n",
        "# Subfolder to store all copied frames in a flat format (not per phase or per video)\n",
        "frames_out_root = os.path.join(output_root, 'Frames')\n",
        "os.makedirs(frames_out_root, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n",
        "# List to store all rows of the CSV (each row = one frame and its label)\n",
        "rows = []\n",
        "\n",
        "# --- PROCESS EACH VIDEO ANNOTATION FILE ---\n",
        "\n",
        "# Loop through all annotation files (one per video)\n",
        "for anno_file in glob.glob(f'{annotation_folder}/*_annotation.txt'):\n",
        "\n",
        "    # Extract the video ID from the filename (e.g., \"1_1\" from \"1_1_annotation.txt\")\n",
        "    video_id = os.path.basename(anno_file).replace('_annotation.txt', '')\n",
        "\n",
        "    # Load the annotation file as a DataFrame\n",
        "    df = pd.read_csv(anno_file, sep='\\t')\n",
        "\n",
        "    # Get the 'Phase' labels and resample them (e.g., every 120th label)\n",
        "    phases = df['Phase'].tolist()[::resample_rate]\n",
        "\n",
        "    # Get corresponding resampled frame paths\n",
        "    frame_paths = sorted(glob.glob(f\"{frames_folder}/{video_id}/*.jpg\"))\n",
        "\n",
        "    # Ensure that the number of frames matches the number of labels\n",
        "    if len(phases) != len(frame_paths):\n",
        "        print(f\"‚ö†Ô∏è Skipping {video_id}: mismatched {len(phases)} labels vs {len(frame_paths)} frames\")\n",
        "        continue\n",
        "\n",
        "    # Process each (frame, label) pair\n",
        "    for i, (frame_path, phase) in enumerate(zip(frame_paths, phases)):\n",
        "\n",
        "        # Create a consistent new filename (e.g., \"1_1_frame_0004.jpg\")\n",
        "        new_frame_name = f\"{video_id}_frame_{i:04d}.jpg\"\n",
        "\n",
        "        # Path where this frame will be copied\n",
        "        new_frame_path = os.path.join(frames_out_root, new_frame_name)\n",
        "\n",
        "        # Copy the frame to the output folder\n",
        "        shutil.copy(frame_path, new_frame_path)\n",
        "\n",
        "        # Create a new row entry for the CSV file\n",
        "        rows.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"frame_name\": new_frame_name,\n",
        "            \"path\": new_frame_path,\n",
        "            \"label\": phase\n",
        "        })\n",
        "\n",
        "# --- SAVE THE CSV FILE ---\n",
        "\n",
        "# Convert the list of rows to a DataFrame\n",
        "df_out = pd.DataFrame(rows)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df_out.to_csv(os.path.join(output_root, 'annotations.csv'), index=False)\n",
        "\n",
        "# Final confirmation message\n",
        "print(f\"‚úÖ CSV-style index saved in {output_root}/annotations.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation"
      ],
      "metadata": {
        "id": "ROlMVoKsT4jf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eDfcEIV_5C9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to CSV structure\n",
        "csv_root = 'MISAW_csv'\n",
        "csv_file = os.path.join(csv_root, 'annotations.csv')\n",
        "frames_folder = os.path.join(csv_root, 'Frames')\n",
        "\n",
        "# Check existence\n",
        "assert os.path.exists(csv_file), \"‚ùå annotations.csv not found\"\n",
        "assert os.path.isdir(frames_folder), \"‚ùå Frames folder not found\"\n",
        "\n",
        "# Load CSV and validate structure\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Check column names\n",
        "expected_cols = {'video_id', 'frame_name', 'path', 'label'}\n",
        "missing_cols = expected_cols - set(df.columns)\n",
        "assert not missing_cols, f\"‚ùå Missing columns in CSV: {missing_cols}\"\n",
        "\n",
        "# Check path existence\n",
        "missing = df[~df['path'].apply(os.path.exists)]\n",
        "\n",
        "# Print summary\n",
        "print(f\"‚úÖ CSV loaded with {len(df)} rows\")\n",
        "print(f\"üîç Sample row:\\n{df.iloc[0]}\")\n",
        "if not missing.empty:\n",
        "    print(f\"‚ö†Ô∏è {len(missing)} listed frames are missing from disk\")\n",
        "else:\n",
        "    print(\"‚úÖ All listed frame paths exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d376771"
      },
      "source": [
        "### üìÅ Converting to COCO-style Format\n",
        "This section creates a new structure suitable for COCO-style datasets used in deep learning. It includes:\n",
        "```\n",
        "MISAW_coco/\n",
        "  ‚îú‚îÄ‚îÄ Frames/\n",
        "  ‚îÇ     ‚îî‚îÄ‚îÄ video1_frame_0000.jpg\n",
        "  ‚îú‚îÄ‚îÄ annotations.json\n",
        "  ‚îî‚îÄ‚îÄ phase_to_id.json\n",
        "```\n",
        "This format is helpful for multi-label or object detection tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doH6AaZW_LQa"
      },
      "outputs": [],
      "source": [
        "import os           # For file and directory operations\n",
        "import glob         # For finding files using wildcard patterns\n",
        "import pandas as pd # For reading annotation files\n",
        "import json         # For saving annotations in JSON format\n",
        "import shutil       # For copying images\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "resample_rate = 120  # Match this with your frame extraction step size\n",
        "\n",
        "# Folder where annotation .txt files are stored\n",
        "annotation_folder = 'MISAW/train/Procedural decription'\n",
        "\n",
        "# Folder where frames are stored (e.g., MISAW/train/Frames/1_1/frame_0000.jpg)\n",
        "frames_folder = 'MISAW/train/Frames'\n",
        "\n",
        "# Output root folder for the new COCO-style dataset\n",
        "output_root = 'MISAW_coco'\n",
        "\n",
        "# Inside this, all frames will be copied to one flat \"Frames\" folder\n",
        "frames_out_root = os.path.join(output_root, 'Frames')\n",
        "os.makedirs(frames_out_root, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "# --- STEP 1: COLLECT UNIQUE PHASES FROM ALL ANNOTATIONS ---\n",
        "\n",
        "# Set to store all phase names (e.g., Idle, Suturing, etc.)\n",
        "all_phases = set()\n",
        "\n",
        "# Loop through each annotation file and collect phase names\n",
        "for anno_file in glob.glob(f'{annotation_folder}/*_annotation.txt'):\n",
        "    df = pd.read_csv(anno_file, sep='\\t')  # Load tab-separated .txt file as a DataFrame\n",
        "    all_phases.update(df['Phase'].unique())  # Add unique phases to the set\n",
        "\n",
        "# Create dictionaries to map phases to integer IDs and back\n",
        "phase_to_id = {p: i for i, p in enumerate(sorted(all_phases))}  # e.g., \"Suturing\" ‚Üí 2\n",
        "id_to_phase = {i: p for p, i in phase_to_id.items()}            # e.g., 2 ‚Üí \"Suturing\"\n",
        "\n",
        "# --- STEP 2: BUILD JSON ENTRIES AND COPY FRAMES ---\n",
        "\n",
        "entries = []  # This will hold all annotation entries\n",
        "\n",
        "# Process each video annotation\n",
        "for anno_file in glob.glob(f'{annotation_folder}/*_annotation.txt'):\n",
        "    video_id = os.path.basename(anno_file).replace('_annotation.txt', '')  # e.g., \"1_1\"\n",
        "\n",
        "    # Read annotation file\n",
        "    df = pd.read_csv(anno_file, sep='\\t')\n",
        "\n",
        "    # Resample phase labels to match saved frames (e.g., take every 120th label)\n",
        "    phases = df['Phase'].tolist()[::resample_rate]\n",
        "\n",
        "    # Get list of resampled frame image paths\n",
        "    frame_dir = os.path.join(frames_folder, video_id)\n",
        "    frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "\n",
        "    # Ensure the number of frames and labels match\n",
        "    if len(phases) != len(frame_paths):\n",
        "        print(f\"‚ö†Ô∏è Skipping {video_id}: mismatched {len(phases)} labels vs {len(frame_paths)} frames\")\n",
        "        continue\n",
        "\n",
        "    # Loop over each frame-label pair\n",
        "    for i, (frame_path, phase) in enumerate(zip(frame_paths, phases)):\n",
        "        # Create a new frame name (e.g., \"1_1_frame_0003.jpg\")\n",
        "        new_frame_name = f\"{video_id}_frame_{i:04d}.jpg\"\n",
        "\n",
        "        # Full destination path for the copied frame\n",
        "        new_frame_path = os.path.join(frames_out_root, new_frame_name)\n",
        "\n",
        "        # Copy frame to the output folder\n",
        "        shutil.copy(frame_path, new_frame_path)\n",
        "\n",
        "        # Add a dictionary entry for this frame in COCO-style format\n",
        "        entries.append({\n",
        "            \"video\": video_id,\n",
        "            \"frame\": new_frame_name,\n",
        "            \"path\": new_frame_path,\n",
        "            \"label\": phase,\n",
        "            \"label_id\": phase_to_id[phase]\n",
        "        })\n",
        "\n",
        "# --- STEP 3: SAVE TO DISK ---\n",
        "\n",
        "# Save all frame annotations to a JSON file\n",
        "with open(os.path.join(output_root, 'annotations.json'), 'w') as f:\n",
        "    json.dump(entries, f, indent=2)\n",
        "\n",
        "# Save the phase-to-ID mapping for future use\n",
        "with open(os.path.join(output_root, 'phase_to_id.json'), 'w') as f:\n",
        "    json.dump(phase_to_id, f, indent=2)\n",
        "\n",
        "# Final confirmation\n",
        "print(f\"‚úÖ COCO-style structure created in {output_root}/ with {len(entries)} annotated frames.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation"
      ],
      "metadata": {
        "id": "HfezDVWWTz2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to COCO folder\n",
        "coco_root = 'MISAW_coco'\n",
        "json_path = os.path.join(coco_root, 'annotations.json')\n",
        "frames_folder = os.path.join(coco_root, 'Frames')\n",
        "\n",
        "# Check paths\n",
        "assert os.path.exists(json_path), \"‚ùå annotations.json not found\"\n",
        "assert os.path.isdir(frames_folder), \"‚ùå Frames folder not found\"\n",
        "\n",
        "# Load and check content\n",
        "with open(json_path, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "assert isinstance(annotations, list), \"‚ùå annotations.json should contain a list\"\n",
        "assert 'frame' in annotations[0] and 'label' in annotations[0], \"‚ùå Missing keys in annotation entries\"\n",
        "\n",
        "# Check file existence and print sample\n",
        "missing = [ann for ann in annotations if not os.path.exists(ann['path'])]\n",
        "print(f\"‚úÖ {len(annotations)} total entries\")\n",
        "print(f\"üîç Sample entry:\\n{annotations[0]}\")\n",
        "print(f\"‚úÖ Found {len(annotations) - len(missing)} valid frame paths\")\n",
        "if missing:\n",
        "    print(f\"‚ö†Ô∏è {len(missing)} frames listed but not found on disk\")\n"
      ],
      "metadata": {
        "id": "j33g3owBK2ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MISAWDataset(Dataset):\n",
        "    def __init__(self, annotations, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations[idx]\n",
        "        # image in gray scale\n",
        "        image = Image.open(item['path']).convert('L')\n",
        "        # get the label id: 0,1,...\n",
        "        label = item['label_id']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "_u-X4zOx0V_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load annotations from JSON\n",
        "with open('MISAW_coco/annotations.json') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Create dataset object with optional transforms and resize to 224√ó224\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(),         # Ensures image is 1-channel\n",
        "    transforms.Normalize((0.5,), (0.5,)),  # Adjust if you use RGB images ‚Üí (0.5, 0.5, 0.5)\n",
        "    transforms.Resize((128, 128))\n",
        "])\n",
        "\n",
        "dataset = MISAWDataset(annotations, transform=transform)\n",
        "\n",
        "# Split dataset (80% train, 20% val)\n",
        "train_len = int(len(dataset) * 0.8)\n",
        "val_len = len(dataset) - train_len\n",
        "train_set, val_set = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "# Define dataloaders\n",
        "BATCH_SIZE = 64        # Number of samples processed before the model is updated (per iteration)\n",
        "NUM_WORKERS = 4        # Number of subprocesses to use for data loading (0 means load in main process)\n",
        "LR = 1e-3              # Learning rate used by the optimizer (how big the update steps are)\n",
        "EPOCHS = 100             # Number of complete passes through the training dataset\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "aryRVmIJ0YxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Visualize examples of each digit\n",
        "classes = list(range(3))\n",
        "samples_per_class = {c: None for c in classes}\n",
        "\n",
        "for img, label in dataset:\n",
        "    if samples_per_class[label] is None:\n",
        "        samples_per_class[label] = img\n",
        "    if all(v is not None for v in samples_per_class.values()):\n",
        "        break\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(samples_per_class[i].squeeze(0), cmap=\"gray\")\n",
        "    ax.set_title(f\"Class {i}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TYCDSoHq0ZfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Install dependencies\n",
        "!pip install pytorch-lightning -q\n",
        "!pip install torchmetrics -q"
      ],
      "metadata": {
        "id": "z3aJ750_13MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3__mJ2iK15Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class implements a simple Convolutional Neural Network (CNN)\n",
        "# using the PyTorch Lightning framework, which simplifies training, validation,\n",
        "# and testing loops by abstracting away boilerplate.\n",
        "\n",
        "class LitCNN(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()  # Saves arguments to `hparams` for reproducibility and checkpointing\n",
        "\n",
        "        # Define the CNN architecture using nn.Sequential for readability.\n",
        "        # This network is designed for grayscale (1-channel) 28x28 images (e.g., MNIST).\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # ‚Üí [32, 128, 128]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),                           # ‚Üí [32, 64, 64]\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # ‚Üí [64, 64, 64]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),                           # ‚Üí [64, 32, 32]\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),# ‚Üí [128, 32, 32]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),                           # ‚Üí [128, 16, 16]\n",
        "\n",
        "            nn.Flatten(),                                          # ‚Üí [128 * 16 * 16] = [32768]\n",
        "            nn.Linear(128 * 16 * 16, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 3)                                      # 3-class classification\n",
        "        )\n",
        "\n",
        "        # Accuracy metric for classification, supports multiclass tasks\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the model\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Use the Adam optimizer with a learning rate defined externally (LR)\n",
        "        return torch.optim.Adam(self.parameters(), lr=LR)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # One training step:\n",
        "        # - batch contains (x, y): input images and their corresponding labels\n",
        "        # - Compute predictions and loss\n",
        "        # - Log the loss and accuracy\n",
        "        x, y = batch\n",
        "        y_hat = self(x)                                 # Forward pass\n",
        "        loss = nn.CrossEntropyLoss()(y_hat, y)          # Compute cross-entropy loss\n",
        "        acc = self.accuracy(y_hat, y)                   # Compute accuracy\n",
        "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)  # Log loss for epoch\n",
        "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)    # Log accuracy for epoch\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Validation step is similar to training, but usually no gradients are computed\n",
        "        # Metrics are logged for monitoring generalization\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
        "        acc = self.accuracy(y_hat, y)\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # Test step is similar to validation, used for final evaluation\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
        "        acc = self.accuracy(y_hat, y)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_acc\", acc)\n"
      ],
      "metadata": {
        "id": "RELwc50t1osH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# üöÇ Training\n",
        "model = LitCNN()\n",
        "\n",
        "# Initialize TensorBoardLogger\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"resnet_tcn_model\")\n",
        "\n",
        "# Initialize EarlyStopping and ModelCheckpoint callbacks\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=5, mode=\"min\", verbose=True\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\", mode=\"min\", save_top_k=1, verbose=True, dirpath=\"checkpoints/\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(max_epochs=EPOCHS,\n",
        "                  accelerator=\"auto\",\n",
        "                  devices=\"auto\",\n",
        "                  log_every_n_steps=1,  # ‚úÖ log at every step\n",
        "                  logger=logger,  # Log to TensorBoard\n",
        "                  callbacks=[early_stop_callback, checkpoint_callback]  # Add callbacks here\n",
        ")\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "nwC-GWvZ10pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aO3eLmqa2IMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}